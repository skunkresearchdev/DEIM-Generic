{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "572f1388",
   "metadata": {},
   "source": [
    "# Extract CVAT ZIP and create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870c7631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 'new_thermal_under.zip' to '/home/hidara/Documents/datasets/temp_dataset'\n",
      "Warning: Number of image files and label files do not match.\n",
      "Missing labels for 0.0% images: \n",
      "Successfully created YOLO dataset structure at: /home/hidara/Documents/datasets/yolo_dataset_under\n",
      "Train images and labels in: /home/hidara/Documents/datasets/yolo_dataset_under/train/images and /home/hidara/Documents/datasets/yolo_dataset_under/train/labels\n",
      "Test images and labels in: /home/hidara/Documents/datasets/yolo_dataset_under/val/images and /home/hidara/Documents/datasets/yolo_dataset_under/val/labels\n",
      "data.yaml created at: /home/hidara/Documents/datasets/yolo_dataset_under/data.yaml\n",
      "train.txt created at: /home/hidara/Documents/datasets/yolo_dataset_under/train.txt\n",
      "val.txt created at: /home/hidara/Documents/datasets/yolo_dataset_under/val.txt\n",
      "Found 3568 annotation files. Starting conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3568/3568 [00:03<00:00, 1002.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted dataset and saved to /home/hidara/Documents/datasets/yolo_dataset_under/annotations/instances_train.json\n",
      "Found 804 annotation files. Starting conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 804/804 [00:00<00:00, 1136.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted dataset and saved to /home/hidara/Documents/datasets/yolo_dataset_under/annotations/instances_val.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import zipfile\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CocoOutput:\n",
    "    info: dict = field(\n",
    "        default_factory=lambda: {\"description\": \"Converted YOLO dataset\"}\n",
    "    )\n",
    "    licenses: list = field(default_factory=lambda: [])\n",
    "    images: list = field(default_factory=lambda: [])\n",
    "    annotations: list = field(default_factory=lambda: [])\n",
    "    categories: list = field(default_factory=lambda: [])\n",
    "\n",
    "\n",
    "def yolo_to_coco(yolo_dir, images_dir, output_json_path, class_names):\n",
    "    \"\"\"\n",
    "    Converts YOLO format annotations to COCO format JSON.\n",
    "\n",
    "    Args:\n",
    "        yolo_dir (str): Path to the directory containing YOLO .txt files.\n",
    "        images_dir (str): Path to the directory containing corresponding image files (.jpg only).\n",
    "        output_json_path (str): Path to save the output COCO JSON file.\n",
    "        class_names (list): A list of class names, in order corresponding to YOLO class IDs.\n",
    "    \"\"\"\n",
    "    yolo_path = Path(yolo_dir)\n",
    "    images_path = Path(images_dir)\n",
    "    output_path = Path(output_json_path)\n",
    "\n",
    "    coco_output = CocoOutput()\n",
    "\n",
    "    # Populate categories\n",
    "    for i, name in enumerate(class_names):\n",
    "        coco_output.categories.append({\n",
    "            \"id\": i + 1,  # COCO category IDs typically start from 1\n",
    "            \"name\": name,\n",
    "            \"supercategory\": name,  # Or a more general category if applicable\n",
    "        })\n",
    "\n",
    "    image_id_counter = 1\n",
    "    annotation_id_counter = 1\n",
    "\n",
    "    yolo_files = list(yolo_path.glob(\"*.txt\"))\n",
    "    if not yolo_files:\n",
    "        print(f\"Warning: No YOLO annotation files found in {yolo_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(yolo_files)} annotation files. Starting conversion...\")\n",
    "\n",
    "    for yolo_file in tqdm(yolo_files):\n",
    "        base_filename = yolo_file.stem\n",
    "        image_path = images_path / f\"{base_filename}.jpg\"\n",
    "        if not image_path.exists():\n",
    "            image_path = images_path / f\"{base_filename}.png\"\n",
    "            \n",
    "\n",
    "        if not image_path.exists():\n",
    "            print(\n",
    "                f\"Warning: Could not find corresponding image for {yolo_file.name}. Skipping.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Read image to get dimensions\n",
    "            image = cv2.imread(str(image_path))\n",
    "            if image is None:\n",
    "                print(f\"Warning: Failed to read image {image_path}. Skipping.\")\n",
    "                continue\n",
    "            img_height, img_width, _ = image.shape\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading image {image_path}: {e}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Add image entry\n",
    "        image_entry = {\n",
    "            \"id\": image_id_counter,\n",
    "            \"file_name\": str(image_path.absolute()),\n",
    "            \"width\": img_width,\n",
    "            \"height\": img_height,\n",
    "        }\n",
    "        coco_output.images.append(image_entry)\n",
    "\n",
    "        # Process annotations for this image\n",
    "        try:\n",
    "            with open(yolo_file, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error reading annotation file {yolo_file.name}: {e}. Skipping image.\"\n",
    "            )\n",
    "            # Remove the image entry if annotation reading fails\n",
    "            coco_output.images.pop()\n",
    "            continue\n",
    "\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 5:\n",
    "                print(\n",
    "                    f\"Warning: Invalid line format in {yolo_file.name}: '{line.strip()}'. Skipping line.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                class_id = int(parts[0])\n",
    "                x_center_norm = float(parts[1])\n",
    "                y_center_norm = float(parts[2])\n",
    "                width_norm = float(parts[3])\n",
    "                height_norm = float(parts[4])\n",
    "\n",
    "                if not (0 <= class_id < len(class_names)):\n",
    "                    print(\n",
    "                        f\"Warning: Invalid class ID {class_id} in {yolo_file.name}. Skipping line.\"\n",
    "                    )\n",
    "                    continue\n",
    "                if not (\n",
    "                    0 <= x_center_norm <= 1\n",
    "                    and 0 <= y_center_norm <= 1\n",
    "                    and 0 <= width_norm <= 1\n",
    "                    and 0 <= height_norm <= 1\n",
    "                ):\n",
    "                    print(\n",
    "                        f\"Warning: Invalid coordinates/dimensions in {yolo_file.name}: {parts[1:]}. Skipping line.\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            except ValueError:\n",
    "                print(\n",
    "                    f\"Warning: Non-numeric data found in {yolo_file.name}: '{line.strip()}'. Skipping line.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Convert to COCO bbox format [x_min, y_min, width, height]\n",
    "            abs_width = width_norm * img_width\n",
    "            abs_height = height_norm * img_height\n",
    "            x_center_abs = x_center_norm * img_width\n",
    "            y_center_abs = y_center_norm * img_height\n",
    "            x_min = max(0.0, x_center_abs - abs_width / 2)\n",
    "            y_min = max(0.0, y_center_abs - abs_height / 2)\n",
    "            # Ensure width and height are positive and box stays within image bounds\n",
    "            abs_width = min(abs_width, img_width - x_min)\n",
    "            abs_height = min(abs_height, img_height - y_min)\n",
    "\n",
    "            if abs_width <= 0 or abs_height <= 0:\n",
    "                print(\n",
    "                    f\"Warning: Degenerate box calculated for line '{line.strip()}' in {yolo_file.name}. Skipping.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            area = abs_width * abs_height\n",
    "            coco_category_id = class_id + 1  # Map YOLO 0-based ID to COCO 1-based ID\n",
    "\n",
    "            annotation_entry = {\n",
    "                \"id\": annotation_id_counter,\n",
    "                \"image_id\": image_id_counter,\n",
    "                \"category_id\": coco_category_id,\n",
    "                \"bbox\": [x_min, y_min, abs_width, abs_height],\n",
    "                \"area\": area,\n",
    "                \"iscrowd\": 0,\n",
    "                \"segmentation\": [],  # Add empty segmentation list\n",
    "            }\n",
    "            coco_output.annotations.append(annotation_entry)\n",
    "            annotation_id_counter += 1\n",
    "\n",
    "        image_id_counter += 1\n",
    "\n",
    "    # Save COCO JSON file\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(coco_output.__dict__, f, indent=4)\n",
    "        print(f\"Successfully converted dataset and saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing JSON file {output_path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_narrow_predictions():\n",
    "    \"\"\"Remove any predictions where the base is much narrower than the height.\n",
    "    This will ensure we dont train using wheels/hubs that are not at least 1/3 of the height.\n",
    "    This is a heuristic to remove narrow predictions that are likely move train the model incorrectly.\n",
    "    \"\"\"\n",
    "    jpgs = []\n",
    "    deleted_fully = 0\n",
    "    some_deleted = 0\n",
    "    print()\n",
    "    for txtfile in yolo_dataset_path.rglob(\"**/labels/*.txt\"):\n",
    "        data = Path(txtfile).read_text().strip().split(\"\\n\")\n",
    "\n",
    "        newlines = []\n",
    "        for line in data:\n",
    "            if line.strip():\n",
    "                values = line.strip().split()\n",
    "                class_id = int(values[0])\n",
    "                x_center = float(values[1])\n",
    "                y_center = float(values[2])\n",
    "                width_normalized = float(values[3])\n",
    "                height_normalized = float(values[4])\n",
    "\n",
    "                # Calculate normalized area\n",
    "                normalized_area = width_normalized * height_normalized\n",
    "\n",
    "                # Calculate ratio of length to width\n",
    "                ratio = width_normalized / height_normalized\n",
    "\n",
    "                if ratio > 0.335:\n",
    "                    newlines.append(line.strip())\n",
    "\n",
    "                else:\n",
    "                    jpgs.append((ratio, str(txtfile)))\n",
    "\n",
    "        if len(newlines) != len(data):\n",
    "            if len(newlines) == 0:\n",
    "                print(f\"File: {txtfile} has no valid lines\")\n",
    "                txtfile.unlink()\n",
    "                deleted_fully += 1\n",
    "            else:\n",
    "                \"\\n\".join(newlines)\n",
    "                txtfile.write_text(\"\\n\".join(newlines))\n",
    "                some_deleted += 1\n",
    "                print(txtfile)\n",
    "                print(f\"\\tClass ID: {class_id}\")\n",
    "                print(f\"\\t\\tNormalized Width: \\t{width_normalized:.2f}\")\n",
    "                print(f\"\\t\\tNormalized Height: \\t{height_normalized:.2f}\")\n",
    "                print(f\"\\t\\tNormalized Area: \\t{normalized_area:.2f}\")\n",
    "                print(f\"\\t\\tRatio (Length/Width): \\t{ratio:.2f}\")\n",
    "\n",
    "    print(\n",
    "        f\"{deleted_fully} files fully removed\\n{some_deleted} files had at least one entry removed\"\n",
    "    )\n",
    "    # jpg = sorted(jpgs)\n",
    "    # print(f\"Total number of images: {len(jpg)}\")\n",
    "\n",
    "\n",
    "def extract_zip(zip_path, extract_path):\n",
    "    \"\"\"Extracts the contents of a zip file to a specified directory.\"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        print(f\"Successfully extracted '{zip_path}' to '{extract_path}'\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Zip file '{zip_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during extraction: {e}\")\n",
    "\n",
    "\n",
    "def create_yolo_dataset(\n",
    "    temp_dataset_path: Path,\n",
    "    yolo_dataset_path: Path,\n",
    "    train_ratio=0.9,\n",
    "    inc_no_dets: bool = True,\n",
    "):\n",
    "    \"\"\"Creates a YOLO dataset structure with train/test split.\"\"\"\n",
    "    test_ratio = 1 - train_ratio\n",
    "\n",
    "    # Create the new YOLO dataset directory structure\n",
    "    yolo_train_images_path = yolo_dataset_path / \"train/images\"\n",
    "    yolo_train_labels_path = yolo_dataset_path / \"train/labels\"\n",
    "    yolo_test_images_path = yolo_dataset_path / \"val/images\"\n",
    "    yolo_test_labels_path = yolo_dataset_path / \"val/labels\"\n",
    "\n",
    "    yolo_train_images_path.mkdir(parents=True, exist_ok=True)\n",
    "    yolo_train_labels_path.mkdir(parents=True, exist_ok=True)\n",
    "    yolo_test_images_path.mkdir(parents=True, exist_ok=True)\n",
    "    yolo_test_labels_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Get all image and label files from the base dataset\n",
    "    temp_images_path = temp_dataset_path / \"images/train\"\n",
    "    temp_labels_path = temp_dataset_path / \"labels/train\"\n",
    "\n",
    "    if not temp_images_path.is_dir() or not temp_labels_path.is_dir():\n",
    "        print(\n",
    "            f\"Error: '{temp_images_path}' or '{temp_labels_path}' not found in the base dataset.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    image_files = list(sorted(temp_images_path.glob(\"*\")))\n",
    "    label_files = list(sorted(temp_labels_path.glob(\"*\")))\n",
    "\n",
    "    if len(image_files) != len(label_files):\n",
    "        print(\"Warning: Number of image files and label files do not match.\")\n",
    "\n",
    "    # Create a list of (image_path, label_path) pairs\n",
    "    data_pairs = []\n",
    "    missing_labels = []\n",
    "    for image_file in sorted(image_files):\n",
    "        label_file_name = image_file.stem + \".txt\"\n",
    "        corresponding_label_file = temp_labels_path / label_file_name\n",
    "\n",
    "        if not corresponding_label_file.exists():\n",
    "            if inc_no_dets:\n",
    "                corresponding_label_file: Path\n",
    "                corresponding_label_file.write_text(\"\")\n",
    "                label_files.append(corresponding_label_file)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        data_pairs.append((image_file, corresponding_label_file))\n",
    "\n",
    "    print(\n",
    "        f\"Missing labels for {len(missing_labels) / len(image_files) * 100:.1f}% images: \"\n",
    "    )\n",
    "\n",
    "    random.shuffle(data_pairs)\n",
    "    split_index = int(len(data_pairs) * train_ratio)\n",
    "    train_data = sorted(data_pairs[:split_index])\n",
    "    test_data = sorted(data_pairs[split_index:])\n",
    "\n",
    "    train_image_list = []\n",
    "    train_label_list = []\n",
    "    for image_path, label_path in train_data:\n",
    "        if label_path.read_text() == \"\":\n",
    "            continue\n",
    "        dest_image_path = yolo_train_images_path / image_path.name\n",
    "        dest_label_path = yolo_train_labels_path / label_path.name\n",
    "        shutil.copy2(image_path, dest_image_path)\n",
    "        shutil.copy2(label_path, dest_label_path)\n",
    "        train_image_list.append(str(dest_image_path))\n",
    "        train_label_list.append(str(dest_label_path))\n",
    "\n",
    "    test_image_list = []\n",
    "    test_label_list = []\n",
    "    for image_path, label_path in test_data:\n",
    "        label_path: Path\n",
    "        # if label_path.read_text() == \"\":\n",
    "        #     continue\n",
    "        dest_image_path = yolo_test_images_path / image_path.name\n",
    "        dest_label_path = yolo_test_labels_path / label_path.name\n",
    "        shutil.copy2(image_path, dest_image_path)\n",
    "        shutil.copy2(label_path, dest_label_path)\n",
    "        test_image_list.append(str(dest_image_path))\n",
    "        test_label_list.append(str(dest_label_path))\n",
    "\n",
    "    # Read YAML file\n",
    "    with open(temp_dataset_path / \"data.yaml\", \"r\") as stream:\n",
    "        data_yaml_content = yaml.safe_load(stream)\n",
    "\n",
    "    data_yaml_content[\"val\"] = \"val.txt\"\n",
    "\n",
    "    with open(yolo_dataset_path / \"data.yaml\", \"w\") as f:\n",
    "        yaml.dump(data_yaml_content, f)\n",
    "\n",
    "    with open(yolo_dataset_path / \"train.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(train_image_list))\n",
    "\n",
    "    with open(yolo_dataset_path / \"val.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(test_image_list))\n",
    "\n",
    "    print(f\"Successfully created YOLO dataset structure at: {yolo_dataset_path}\")\n",
    "    print(\n",
    "        f\"Train images and labels in: {yolo_train_images_path} and {yolo_train_labels_path}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Test images and labels in: {yolo_test_images_path} and {yolo_test_labels_path}\"\n",
    "    )\n",
    "    print(f\"data.yaml created at: {yolo_dataset_path / 'data.yaml'}\")\n",
    "    print(f\"train.txt created at: {yolo_dataset_path / 'train.txt'}\")\n",
    "    print(f\"val.txt created at: {yolo_dataset_path / 'val.txt'}\")\n",
    "\n",
    "\n",
    "# --- Main Execution in Jupyter ---\n",
    "DATASET_ROOT = Path(\"/home/hidara/Documents/datasets/yolo_dataset_under/\")\n",
    "TEMP_DATASET = Path(\"/home/hidara/Documents/datasets/temp_dataset/\")\n",
    "# 1. Extract the zip file\n",
    "ZIP_FILE_PATH = \"new_thermal_under.zip\"  # Make sure this file is in the same directory\n",
    "\n",
    "yolo_annotation_directory = (DATASET_ROOT / \"train/labels\").absolute()\n",
    "image_directory = DATASET_ROOT / \"train/images\"\n",
    "output_file = DATASET_ROOT / \"annotations/instances_train.json\"\n",
    "if \"under\" in DATASET_ROOT.name:\n",
    "    classes = [\"brakes\"]  # Replace with your actual class names\n",
    "else:\n",
    "    classes = [\"hub\", \"tyre\"]  # Replace with your actual class names\n",
    "\n",
    "shutil.rmtree(\n",
    "    TEMP_DATASET, ignore_errors=True\n",
    ")  # Remove existing yolo_dataset if it exists\n",
    "extract_zip(ZIP_FILE_PATH, TEMP_DATASET)\n",
    "\n",
    "# 2. Create the YOLO dataset\n",
    "shutil.rmtree(\n",
    "    DATASET_ROOT, ignore_errors=True\n",
    ")  # Remove existing yolo_dataset if it exists\n",
    "create_yolo_dataset(TEMP_DATASET, DATASET_ROOT, inc_no_dets=True)\n",
    "# remove_narrow_predictions()\n",
    "\n",
    "\n",
    "\n",
    "# # --- Example Usage ---\n",
    "# Define your class names in the order they appear in YOLO IDs (0, 1, 2...)\n",
    "\n",
    "yolo_to_coco(yolo_annotation_directory, image_directory, output_file, classes)\n",
    "\n",
    "# Repeat for validation set\n",
    "yolo_annotation_directory_val = DATASET_ROOT / \"val/labels\"\n",
    "image_directory_val = DATASET_ROOT / \"val/images\"\n",
    "output_file_val = DATASET_ROOT / \"annotations/instances_val.json\"\n",
    "yolo_to_coco(\n",
    "    yolo_annotation_directory_val, image_directory_val, output_file_val, classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5dcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89ee1314",
   "metadata": {},
   "source": [
    "# YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35399fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from ultralytics import YOLO\n",
    "from tensorboard import notebook\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "\n",
    "# --- Trainer Class ---\n",
    "class YOLOTrainer:\n",
    "    def __init__(self, model_path, data_path, base_save_dir=\"yolo_runs\"):\n",
    "        self.model_path = model_path\n",
    "        self.data_path = data_path\n",
    "        self.base_save_dir = base_save_dir\n",
    "        self.model = None\n",
    "        self.results = None\n",
    "        self.current_run_dir = None\n",
    "\n",
    "    def load_model(self) -> None:\n",
    "        self.model = YOLO(self.model_path)\n",
    "\n",
    "    def train(self, epochs=50, imgsz=640, **kwargs):\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "\n",
    "        # Create a timestamp for the current run directory\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.current_run_dir = os.path.join(self.base_save_dir, timestamp)\n",
    "\n",
    "        # Train the model with specified parameters and save directory\n",
    "        self.results = self.model.train(\n",
    "            data=self.data_path,\n",
    "            epochs=epochs,\n",
    "            imgsz=imgsz,\n",
    "            project=self.base_save_dir,  # Base directory for runs\n",
    "            name=timestamp,  # Subdirectory name based on timestamp\n",
    "            **kwargs,  # Allow passing other training arguments\n",
    "        )\n",
    "        return self.results\n",
    "\n",
    "    def val(self, **kwargs):\n",
    "        if self.model is None:\n",
    "            print(\n",
    "                \"Model not loaded. Please run train() first or load a model manually.\"\n",
    "            )\n",
    "            return None\n",
    "        return self.model.val(**kwargs)\n",
    "\n",
    "    def predict(self, source, **kwargs):\n",
    "        if self.model is None:\n",
    "            print(\n",
    "                \"Model not loaded. Please run train() first or load a model manually.\"\n",
    "            )\n",
    "            return None\n",
    "        return self.model.predict(source=source, **kwargs)\n",
    "\n",
    "    def export(self, format=\"torchscript\", **kwargs):\n",
    "        if self.model is None:\n",
    "            print(\n",
    "                \"Model not loaded. Please run train() first or load a model manually.\"\n",
    "            )\n",
    "            return None\n",
    "        return self.model.export(format=format, **kwargs)\n",
    "\n",
    "    def tune(self, **kwargs):\n",
    "        if self.model is None:\n",
    "            print(\n",
    "                \"Model not loaded. Please run train() first or load a model manually.\"\n",
    "            )\n",
    "            return None\n",
    "        return self.model.tune(**kwargs)\n",
    "\n",
    "    def info(self):\n",
    "        if self.model is None:\n",
    "            print(\n",
    "                \"Model not loaded. Please run train() first or load a model manually.\"\n",
    "            )\n",
    "            return None\n",
    "        return self.model.info()\n",
    "\n",
    "\n",
    "# --- Usage in the Notebook ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Instantiate the trainer\n",
    "    # latest_models = sorted(\n",
    "    #     Path(\"yolo_runs\").rglob(\"best.pt\"), key=lambda x: x.stat().st_ctime\n",
    "    # )\n",
    "    for batch_size, version in [(32, \"n\")]:\n",
    "        latest_models = sorted(\n",
    "            Path(\"yolo_runs\").rglob(f\"{version}/**/best.pt\"),\n",
    "            key=lambda x: x.stat().st_ctime,\n",
    "        )\n",
    "\n",
    "        # if latest_models:\n",
    "        #     model_name = latest_models[-1].absolute()\n",
    "        # else:\n",
    "        #\n",
    "        model_name = f\"yolo12{version}.pt\"\n",
    "\n",
    "        trainer = YOLOTrainer(\n",
    "            model_path=model_name,  # latest model\n",
    "            data_path=\"/home/hidara/Documents/datasets/yolo_dataset_under/data.yaml\",\n",
    "            base_save_dir=f\"yolo_runs/{version}/\",\n",
    "        )\n",
    "\n",
    "        notebook.start(\"--logdir \" + \"yolo_runs\")\n",
    "\n",
    "        # --- Training with adjustable settings ---\n",
    "        epochs_to_train = 250  # Example: Change the number of epochs\n",
    "        image_size = 512  # Example: Change the image size\n",
    "        batch_size = 32  # Example: Change the batch size\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"using {model_name} for training\")\n",
    "        print(\n",
    "            f\"Starting training with epochs={epochs_to_train}, image size={image_size}, batch size={batch_size}/n/n\"\n",
    "        )\n",
    "        training_results = trainer.train(\n",
    "            epochs=epochs_to_train,\n",
    "            imgsz=image_size,\n",
    "            batch=batch_size,\n",
    "            augment=True,\n",
    "            multi_scale=True,\n",
    "            flipud=0.5,\n",
    "            bgr=0.5,\n",
    "            mixup=0.8,\n",
    "            copy_paste=0.8,\n",
    "            shear=1.0,  # Adjust this value. It's often in degrees.\n",
    "            perspective=0.001,  # Adjust this value. It's often a small factor.\n",
    "            crop_fraction=0.9,\n",
    "        )\n",
    "\n",
    "        if training_results:\n",
    "            print(\"Training completed.\")\n",
    "            print(f\"Results saved in: {trainer.current_run_dir}\")\n",
    "\n",
    "            # --- Running TensorBoard ---\n",
    "            print(\"\\n--- Running TensorBoard ---\")\n",
    "            print(\n",
    "                f\"Navigate to the 'runs' directory within '{trainer.base_save_dir}' to view TensorBoard logs.\"\n",
    "            )\n",
    "            print(\"You can typically start TensorBoard in your terminal using:\")\n",
    "            print(f\"`tensorboard --logdir {trainer.base_save_dir}`\")\n",
    "            print(\"Then open your browser to http://localhost:6006/\")\n",
    "\n",
    "        else:\n",
    "            print(\"Training did not start or encountered an issue.\")\n",
    "\n",
    "        del trainer\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bf9061",
   "metadata": {},
   "source": [
    "# INFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from tools.inference.onnx_inf_super_io import (\n",
    "    main as onnx_main,\n",
    ")  # Renamed to avoid confusion with the main script\n",
    "\n",
    "\n",
    "def process_video(video_path):\n",
    "    \"\"\"\n",
    "    Processes a single video using ONNX inference within a multiprocessing pool.\n",
    "\n",
    "    Args:\n",
    "        video_path (Path): Path to the video file.\n",
    "    \"\"\"\n",
    "    # **Crucially, the ONNX inference initialization happens within this function**\n",
    "    # This ensures each process initializes its own CUDA context if needed.\n",
    "    return onnx_main(\n",
    "        input_path=video_path,  # Ensure path is a string for potential compatibility\n",
    "        onnx_path=Path(\"best_sides.onnx\"),\n",
    "        labels_dict={0: \"Hub\", 1: \"Wheel\"}, #{0: \"Brakes\"},  # \n",
    "        output_dir=Path(\"outputs\"),\n",
    "        batch_size=16,\n",
    "        conf_threshold=0.79,\n",
    "        debug=False,\n",
    "        cuda_device_id=0,  # You might want to manage device IDs per process if using multiple GPUs\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    videos = sorted(Path(\"videos\").glob(\"*c330e_TC0[12]*.mp4\"))\n",
    "\n",
    "    with Pool(8) as p:\n",
    "        p.map(process_video, videos)\n",
    "    # for video_path in videos:\n",
    "    #     process_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from tools.inference.onnx_inf_super_io import (\n",
    "    main as onnx_main,\n",
    ")  # Renamed to avoid confusion with the main script\n",
    "\n",
    "\n",
    "def process_video(video_path):\n",
    "    \"\"\"\n",
    "    Processes a single video using ONNX inference within a multiprocessing pool.\n",
    "\n",
    "    Args:\n",
    "        video_path (Path): Path to the video file.\n",
    "    \"\"\"\n",
    "    # **Crucially, the ONNX inference initialization happens within this function**\n",
    "    # This ensures each process initializes its own CUDA context if needed.\n",
    "    return onnx_main(\n",
    "        input_path=video_path,  # Ensure path is a string for potential compatibility\n",
    "        onnx_path=Path(\"deim_outputs/sides/20250416_204621/best_stg2.onnx\"),\n",
    "        labels_dict={0: \"Hub\", 1: \"Wheel\", 2: \"Wheel\", 3: \"Wheel\"},\n",
    "        output_dir=Path(\"outputs\"),\n",
    "        batch_size=16,\n",
    "        conf_threshold=0.3,\n",
    "        debug=False,\n",
    "        cuda_device_id=0,  # You might want to manage device IDs per process if using multiple GPUs\n",
    "        is_tracking=True\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    videos = sorted(Path(\"videos\").glob(\"*TC0*.mp4\"))\n",
    "\n",
    "    with Pool(8) as p:\n",
    "        p.map(process_video, videos)\n",
    "    # for video_path in videos:\n",
    "    #     process_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44889e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from tools.inference.onnx_inf_super_io import (\n",
    "    main as onnx_main,\n",
    ")  # Renamed to avoid confusion with the main script\n",
    "\n",
    "\n",
    "def process_video(video_path):\n",
    "    \"\"\"\n",
    "    Processes a single video using ONNX inference within a multiprocessing pool.\n",
    "\n",
    "    Args:\n",
    "        video_path (Path): Path to the video file.\n",
    "    \"\"\"\n",
    "    # **Crucially, the ONNX inference initialization happens within this function**\n",
    "    # This ensures each process initializes its own CUDA context if needed.\n",
    "    return onnx_main(\n",
    "        input_path=video_path,  # Ensure path is a string for potential compatibility\n",
    "        onnx_path=Path(\"deim_outputs/under/20250419_124347/best_stg2.onnx\"),\n",
    "        labels_dict={0: \"Brakes\"}, # {0: \"Wheel\", 1: \"Test\"},\n",
    "        output_dir=Path(\"outputs\"),\n",
    "        batch_size=64,\n",
    "        conf_threshold=0.8,\n",
    "        debug=False,\n",
    "        cuda_device_id=0,  # You might want to manage device IDs per process if using multiple GPUs\n",
    "        is_tracking=True\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    videos = sorted(Path(\"videos\").glob(\"*TC03*.mp4\"))\n",
    "\n",
    "    process_video(\"videos/67a2d700554dd5faf24c592b_TC03_cupy.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a43d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo_runs/n/20250416_215957/weights/best.pt\")\n",
    "\n",
    "\n",
    "model.predict(\"videos/66d8c56ea502fd4f902c330e_TC03_cupy.mp4\", save=True, conf=0.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
